# -*- coding: utf-8 -*-
# æ–‡æœ¬ä¿¡æ¯æŒ–æ˜è¯¾ç¨‹è®¾è®¡ï¼šåŸºäºä¸»é¢˜æ¨¡å‹çš„ç½‘ç»œæ–°è¯æ¼”åŒ–åˆ†æ
# ç­çº§ï¼šXXç­ | å­¦å·ï¼šXXXXXX | å§“åï¼šXXX
# æ—¥æœŸï¼š2023å¹´6æœˆ
import matplotlib.font_manager as fm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import networkx as nx
from PIL import Image, ImageDraw, ImageFont
import random
import warnings
import os
import ssl

# ä¿®å¤SSLè¯ä¹¦é—®é¢˜
ssl._create_default_https_context = ssl._create_unverified_context
os.environ['CURL_CA_BUNDLE'] = ''

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.facecolor'] = 'white'

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs("output", exist_ok=True)
def find_chinese_font():
    """æŸ¥æ‰¾ç³»ç»Ÿä¸­å¯ç”¨çš„ä¸­æ–‡å­—ä½“"""
    # å¸¸è§ä¸­æ–‡å­—ä½“åç§°
    chinese_fonts = [
        'SimHei', 'Microsoft YaHei', 'KaiTi', 'SimSun',
        'FangSong', 'STSong', 'STKaiti', 'STFangsong',
        'WenQuanYi Micro Hei', 'Source Han Sans SC',
        'Noto Sans CJK SC', 'WenQuanYi Zen Hei'
    ]

    # è·å–ç³»ç»Ÿæ‰€æœ‰å­—ä½“
    system_fonts = fm.findSystemFonts()
    for font_name in chinese_fonts:
        for font_path in system_fonts:
            if font_name.lower() in os.path.basename(font_path).lower():
                return font_path

    # å¤‡é€‰æ–¹æ¡ˆï¼šæŸ¥æ‰¾åŒ…å«"é»‘ä½“"ã€"å®‹ä½“"ç­‰å…³é”®å­—çš„å­—ä½“
    keywords = ['hei', 'song', 'kai', 'fang', 'st', 'cjk', 'sc', 'chinese', 'ä¸­æ–‡']
    for font_path in system_fonts:
        font_name = os.path.basename(font_path).lower()
        if any(kw in font_name for kw in keywords):
            return font_path
    return None  # æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“


# ======================
# 1. æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†
# ======================

print("æ­¥éª¤1: æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†...")


# ä¿®æ”¹åˆ›å»ºæ•°æ®é›†çš„å‡½æ•°
def create_dataset():
    """åˆ›å»º2022-2025å¹´ä»£è¡¨æ€§æ–°è¯æ•°æ®é›†"""
    data = {
        "keyword": ["å†…å·", "èººå¹³", "PUA", "æ¯’é¸¡æ±¤", "åŸç¥", "å¡å°”è¾¾ä¼ è¯´", "æ‡‚ç‹", "å·å»ºå›½",
                    "å¥¥è§‚æµ·", "è§‰é†’å¹´ä»£", "åŒå‡", "å…ƒå®‡å®™", "ChatGPT", "å­”ä¹™å·±æ–‡å­¦", "00åæ•´é¡¿èŒåœº",
                    "ç”µå­æ¦¨èœ", "ç‹‚é£™", "æ‘BA", "ç‰¹ç§å…µæ—…æ¸¸", "æ˜¾çœ¼åŒ…", "æ­å­", "å¤šå·´èƒºç©¿æ­",
                    "ç¤¾æ", "ç¤¾ç‰›", "æ‘†çƒ‚", "æ¶¦å­¦", "å¤©é€‰æ‰“å·¥äºº", "é›ªç³•åˆºå®¢", "é€€é€€é€€", "æ “Q",
                    "çœŸé¦™", "å‡¡å°”èµ›", "ç»ç»å­", "yyds", "emo", "ç ´é˜²", "èˆ”ç‹—", "æ ç²¾", "ä½›ç³»",
                    # 2022å¹´æ–°å¢
                    "åˆ˜ç•Šå®å¥³å­©", "ç‹å¿ƒå‡Œç”·å­©", "äºŒèˆ…", "å°é•‡åšé¢˜å®¶", "å›¢é•¿", "å˜´æ›¿", "æœäº†ä½ ä¸ªè€å…­",
                    "æ ¸é…¸åª›", "å­¤å‹‡è€…å°å­©", "å¯è¾¾é¸­", "ç¾Šäº†ä¸ªç¾Š", "ç”µå­æœ¨é±¼", "ç§‘æŠ€ä¸ç‹ æ´»", "é€€ğŸ¤ºé€€ğŸ¤ºé€€ğŸ¤º",
                    # 2023å¹´æ–°å¢
                    "iäºº/eäºº", "æ³¼å¤©å¯Œè´µ", "é¥é¥é¢†å…ˆ", "å°Šå˜Ÿå‡å˜Ÿ", "å“ˆåŸºç±³", "æ³°è£¤è¾£", "æŒ–å‘€æŒ–",
                    "å‘½è¿çš„é½¿è½®å¼€å§‹è½¬åŠ¨", "å…¨èŒå„¿å¥³", "è„†çš®å¤§å­¦ç”Ÿ", "å…¬ä¸»è¯·ä¸Šè½¦", "å±±æ²³å¤§å­¦", "è½»èˆŸå·²è¿‡ä¸‡é‡å±±",
                    "è´¨ç–‘ç†è§£æˆä¸º", "é¼ é¼ æ–‡å­¦", "å‘ç–¯æ–‡å­¦", "æµ·åº•æç§‘ç›®ä¸‰", "é…±é¦™æ‹¿é“",
                    # 2024å¹´æ–°å¢
                    "æ–°è´¨ç”Ÿäº§åŠ›", "äººå·¥æ™ºèƒ½+", "ä½ç©ºç»æµ", "æƒ…ç»ªä»·å€¼", "æç¯å®šæŸ", "ç™¾æ—¥èª“å¸ˆ", "é¹…è…¿é˜¿å§¨",
                    "ä¸‰åªç¾Š", "è®¸æ„¿æ± çš„ç‹å…«", "æ³¼å¤©çš„å¯Œè´µ", "ç§¦æœ—ä¸¢ä½œä¸š", "å†œå¤«å±±æ³‰äº‹ä»¶", "èƒ–çŒ«äº‹ä»¶",
                    "ç§‘ç›®ä¸‰èˆè¹ˆ", "å¤©æ°´éº»è¾£çƒ«", "ç‹å©†è¯´åª’", "ææ¡¶è·‘è·¯", "è¹­è€å¼æ¶ˆè´¹", "çˆ±å› æ–¯å¦çš„è„‘å­",
                    # 2025å¹´æ–°å¢
                    "AIä¼´ä¾£", "è„‘æœºæ¥å£", "é‡å­è®¡ç®—", "å¯æ§æ ¸èšå˜", "æ˜Ÿé™…æ—…è¡Œ", "æ•°å­—æ°¸ç”Ÿ", "è™šæ‹Ÿå¶åƒ2.0",
                    "å…ƒå®‡å®™åŠå…¬", "æƒ…ç»ªç»æµ", "åå†…å·è”ç›Ÿ", "æ•°å­—æ¸¸æ°‘", "èµ›åšå…»ç”Ÿ", "åç„¦è™‘è¥é”€", "ç”µå­åŠŸå¾·",
                    "æ²‰æµ¸å¼ä½“éªŒ", "æƒ…ç»ªè‡ªç”±", "æ™ºæ¢°å±æœº", "æ•°æ®èµ„äº§"],
        "year": [2020, 2021, 2020, 2020, 2020, 2023, 2020, 2020, 2020, 2021, 2021, 2021, 2023, 2023, 2022,
                 2022, 2023, 2023, 2023, 2023, 2023, 2023, 2021, 2021, 2022, 2022, 2022, 2022, 2022, 2022,
                 2020, 2020, 2021, 2021, 2021, 2021, 2019, 2019, 2018,
                 # 2022å¹´æ–°å¢
                 2022, 2022, 2022, 2022, 2022, 2022, 2022,
                 2022, 2022, 2022, 2022, 2022, 2022, 2022,
                 # 2023å¹´æ–°å¢
                 2023, 2023, 2023, 2023, 2023, 2023, 2023,
                 2023, 2023, 2023, 2023, 2023, 2023,
                 2023, 2023, 2023, 2023, 2023,
                 # 2024å¹´æ–°å¢
                 2024, 2024, 2024, 2024, 2024, 2024, 2024,
                 2024, 2024, 2024, 2024, 2024, 2024,
                 2024, 2024, 2024, 2024, 2024, 2024,
                 # 2025å¹´æ–°å¢
                 2025, 2025, 2025, 2025, 2025, 2025, 2025,
                 2025, 2025, 2025, 2025, 2025, 2025, 2025,
                 2025, 2025, 2025, 2025],
        "frequency": [850, 920, 420, 380, 1200, 850, 780, 650, 320, 560, 720, 890, 1500, 680, 750,
                      620, 980, 530, 710, 640, 580, 590, 520, 510, 680, 590, 540, 620, 580, 550,
                      820, 780, 950, 1100, 920, 850, 480, 460, 420,
                      # 2022å¹´æ–°å¢
                      720, 680, 890, 750, 820, 630, 710,
                      590, 780, 930, 850, 1100, 670, 790,
                      # 2023å¹´æ–°å¢
                      820, 950, 1200, 780, 860, 710, 890,
                      730, 680, 920, 640, 850, 990,
                      710, 650, 780, 1050, 1150,
                      # 2024å¹´æ–°å¢
                      980, 1100, 850, 920, 1050, 780, 690,
                      950, 820, 890, 1020, 980, 1150,
                      1080, 920, 870, 790, 830, 760,
                      # 2025å¹´æ–°å¢
                      850, 920, 780, 690, 750, 820, 980,
                      890, 950, 1020, 870, 790, 850, 930,
                      880, 960, 820, 770],
        "category": ["ç¤¾ä¼š", "ç¤¾ä¼š", "ç¤¾ä¼š", "æ–‡åŒ–", "å¨±ä¹", "å¨±ä¹", "æ”¿æ²»", "æ”¿æ²»", "æ”¿æ²»", "æ–‡åŒ–", "æ•™è‚²", "ç§‘æŠ€",
                     "ç§‘æŠ€", "æ–‡åŒ–", "ç¤¾ä¼š", "å¨±ä¹", "å¨±ä¹", "ä½“è‚²", "ç”Ÿæ´»", "ç”Ÿæ´»", "ç”Ÿæ´»", "æ—¶å°š",
                     "å¿ƒç†", "å¿ƒç†", "æ€åº¦", "æ€åº¦", "å·¥ä½œ", "ç”Ÿæ´»", "ç½‘ç»œ", "ç½‘ç»œ",
                     "æ€åº¦", "æ€åº¦", "ç½‘ç»œ", "ç½‘ç»œ", "å¿ƒç†", "å¿ƒç†", "ç½‘ç»œ", "ç½‘ç»œ", "æ€åº¦",
                     # 2022å¹´æ–°å¢
                     "å¥åº·", "å¨±ä¹", "ç¤¾ä¼š", "æ•™è‚²", "ç”Ÿæ´»", "ç½‘ç»œ", "æ¸¸æˆ",
                     "ç¤¾ä¼š", "å¿ƒç†", "å¨±ä¹", "å¨±ä¹", "æ¸¸æˆ", "ç§‘æŠ€", "ç”Ÿæ´»",
                     # 2023å¹´æ–°å¢
                     "å¿ƒç†", "ç»æµ", "ç§‘æŠ€", "ç½‘ç»œ", "å¨±ä¹", "æ€åº¦", "å¨±ä¹",
                     "æ€åº¦", "ç¤¾ä¼š", "æ•™è‚²", "ç”Ÿæ´»", "æ•™è‚²", "æ–‡åŒ–",
                     "æ€åº¦", "å¿ƒç†", "æ–‡åŒ–", "å¨±ä¹", "ç”Ÿæ´»",
                     # 2024å¹´æ–°å¢
                     "ç»æµ", "ç§‘æŠ€", "ç»æµ", "å¿ƒç†", "ç¤¾ä¼š", "æ•™è‚²", "ç”Ÿæ´»",
                     "å¨±ä¹", "ç”Ÿæ´»", "ç»æµ", "æ•™è‚²", "ç¤¾ä¼š", "ç¤¾ä¼š",
                     "å¨±ä¹", "ç”Ÿæ´»", "ç¤¾ä¼š", "å·¥ä½œ", "æ¶ˆè´¹", "å¿ƒç†",
                     # 2025å¹´æ–°å¢
                     "ç§‘æŠ€", "ç§‘æŠ€", "ç§‘æŠ€", "ç§‘æŠ€", "ç§‘æŠ€", "ç§‘æŠ€", "å¨±ä¹",
                     "å·¥ä½œ", "ç»æµ", "ç¤¾ä¼š", "å·¥ä½œ", "å¥åº·", "è¥é”€", "å¿ƒç†",
                     "å¨±ä¹", "å¿ƒç†", "ç§‘æŠ€", "ç»æµ"]
    }

    # ç”Ÿæˆéšæœºæƒ…æ„Ÿå€¼ï¼ˆ-1åˆ°1ä¹‹é—´ï¼‰
    sentiments = []
    for word in data["keyword"]:
        if word in ["å†…å·", "èººå¹³", "PUA", "æ¯’é¸¡æ±¤", "å­”ä¹™å·±æ–‡å­¦", "ç¤¾æ", "æ‘†çƒ‚", "èˆ”ç‹—", "æ ç²¾", "é›ªç³•åˆºå®¢",
                   "æœäº†ä½ ä¸ªè€å…­", "æ ¸é…¸åª›", "æç¯å®šæŸ", "ç™¾æ—¥èª“å¸ˆ", "èƒ–çŒ«äº‹ä»¶", "å†œå¤«å±±æ³‰äº‹ä»¶", "ææ¡¶è·‘è·¯",
                   "é¼ é¼ æ–‡å­¦", "è„†çš®å¤§å­¦ç”Ÿ", "èµ›åšå…»ç”Ÿ", "æ™ºæ¢°å±æœº"]:
            sentiments.append(round(random.uniform(-1, -0.3), 2))  # è´Ÿé¢è¯æ±‡
        elif word in ["åŸç¥", "å¡å°”è¾¾ä¼ è¯´", "è§‰é†’å¹´ä»£", "ChatGPT", "æ‘BA", "å¤šå·´èƒºç©¿æ­", "ç¤¾ç‰›", "çœŸé¦™", "yyds",
                     "åˆ˜ç•Šå®å¥³å­©", "ç‹å¿ƒå‡Œç”·å­©", "å¯è¾¾é¸­", "ç¾Šäº†ä¸ªç¾Š", "ç”µå­æœ¨é±¼", "å°Šå˜Ÿå‡å˜Ÿ", "æŒ–å‘€æŒ–",
                     "å‘½è¿çš„é½¿è½®å¼€å§‹è½¬åŠ¨", "å…¬ä¸»è¯·ä¸Šè½¦", "å±±æ²³å¤§å­¦", "è½»èˆŸå·²è¿‡ä¸‡é‡å±±", "é…±é¦™æ‹¿é“",
                     "æ–°è´¨ç”Ÿäº§åŠ›", "äººå·¥æ™ºèƒ½+", "ä½ç©ºç»æµ", "æƒ…ç»ªä»·å€¼", "é¹…è…¿é˜¿å§¨", "è®¸æ„¿æ± çš„ç‹å…«",
                     "å¤©æ°´éº»è¾£çƒ«", "ç‹å©†è¯´åª’", "AIä¼´ä¾£", "é‡å­è®¡ç®—", "å¯æ§æ ¸èšå˜", "æ˜Ÿé™…æ—…è¡Œ", "æ•°å­—æ°¸ç”Ÿ",
                     "è™šæ‹Ÿå¶åƒ2.0", "å…ƒå®‡å®™åŠå…¬", "æƒ…ç»ªç»æµ", "æƒ…ç»ªè‡ªç”±", "æ•°æ®èµ„äº§"]:
            sentiments.append(round(random.uniform(0.3, 1), 2))  # æ­£é¢è¯æ±‡
        else:
            sentiments.append(round(random.uniform(-0.3, 0.3), 2))  # ä¸­æ€§è¯æ±‡

    data["sentiment"] = sentiments
    df = pd.DataFrame(data)
    df['year'] = pd.to_datetime(df['year'], format='%Y')  # è½¬æ¢ä¸ºdatetimeç±»å‹
    return df
# åˆ›å»ºæ•°æ®é›†
df = create_dataset()
print(f"æ•°æ®é›†åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(df)} æ¡è®°å½•")
print(df.head())

# ä¿å­˜æ•°æ®é›†
df.to_csv("output/new_words_dataset.csv", index=False, encoding='utf-8-sig')
print("æ•°æ®é›†å·²ä¿å­˜åˆ°: output/new_words_dataset.csv")

# ======================
# 2. ä¸»é¢˜å»ºæ¨¡ä¸åˆ†æ
# ======================

print("\næ­¥éª¤2: ä¸»é¢˜å»ºæ¨¡ä¸åˆ†æ...")


def run_topic_modeling(docs):
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(docs)
    # è®­ç»ƒLDAæ¨¡å‹
    lda = LatentDirichletAllocation(n_components=5, random_state=42)
    lda.fit(X)
    topics = lda.transform(X).argmax(axis=1)
    # åˆ›å»ºä¸»é¢˜ä¿¡æ¯DataFrame
    topic_info = pd.DataFrame({
        'Topic': range(lda.n_components),
        'Count': np.bincount(topics),
        'Name': [f"Topic_{i}" for i in range(lda.n_components)]
    })
    feature_names = vectorizer.get_feature_names_out()
    print("\nLDAä¸»é¢˜è¯:")
    lda_topics = []
    for topic_idx, topic in enumerate(lda.components_):
        top_words_idx = topic.argsort()[:-6:-1]
        top_words = [feature_names[i] for i in top_words_idx]
        topic_str = f"ä¸»é¢˜ #{topic_idx + 1}: {', '.join(top_words)}"
        print(topic_str)
        lda_topics.append(topic_str)
        topic_info.loc[topic_idx, 'Words'] = ', '.join(top_words)
    # ä¿å­˜LDAç»“æœ
    with open("output/lda_topics.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(lda_topics))
    print("LDAä¸»é¢˜ç»“æœå·²ä¿å­˜åˆ°: output/lda_topics.txt")
    return None, topics, topic_info


# è¿è¡Œä¸»é¢˜å»ºæ¨¡
docs = df['keyword'].tolist()
topic_model, topics, topic_info = run_topic_modeling(docs)

# å°†ä¸»é¢˜ç»“æœåŠ å…¥æ•°æ®æ¡†
df['topic'] = topics

# æ·»åŠ ä¸»é¢˜åç§°
topic_names = {
    0: "èŒåœºä¸ç¤¾ä¼šç°è±¡",
    1: "å¨±ä¹ä¸æ¸¸æˆ",
    2: "æ”¿æ²»ä¸ç½‘ç»œæ–‡åŒ–",
    3: "ç”Ÿæ´»æ€åº¦",
    4: "æ–°å…´ç§‘æŠ€"
}
df['topic_name'] = df['topic'].map(topic_names)

# ä¿å­˜å®Œæ•´åˆ†ææ•°æ®é›†
df.to_csv("output/full_analysis_dataset.csv", index=False, encoding='utf-8-sig')
print("å®Œæ•´åˆ†ææ•°æ®é›†å·²ä¿å­˜åˆ°: output/full_analysis_dataset.csv")

# ======================
# 3. æœ¬åœ°AIGCæ¨¡æ‹Ÿåˆ†æ
# ======================

print("\næ­¥éª¤3: æœ¬åœ°AIGCæ¨¡æ‹Ÿåˆ†æ...")


def analyze_topic_locally(keywords):
    """æœ¬åœ°æ¨¡æ‹Ÿä¸»é¢˜åˆ†æ"""
    # é¢„å®šä¹‰åˆ†ææ¨¡æ¿
    templates = {
        "èŒåœºä¸ç¤¾ä¼šç°è±¡": [
            f"å…³é”®è¯ '{', '.join(keywords)}' åæ˜ äº†å½“ä»£ç¤¾ä¼šçš„èŒåœºå‹åŠ›å’Œç¤¾ä¼šå¿ƒæ€å˜åŒ–ã€‚",
            "è¿™äº›è¯æ±‡å±•ç°äº†å¹´è½»äººåœ¨èŒåœºç«äº‰ä¸­çš„å¤æ‚å¿ƒæ€ï¼Œæ—¢æœ‰å¯¹é«˜å‹ç¯å¢ƒçš„åæŠ—ï¼Œä¹Ÿæœ‰å¯¹è‡ªæˆ‘ä»·å€¼çš„æ¢ç´¢ã€‚",
            "ä»'å†…å·'åˆ°'èººå¹³'å†åˆ°'00åæ•´é¡¿èŒåœº'ï¼Œä½“ç°äº†ä¸åŒä»£é™…å¯¹å·¥ä½œä»·å€¼è®¤çŸ¥çš„è½¬å˜ã€‚"
        ],
        "å¨±ä¹ä¸æ¸¸æˆ": [
            f"å¨±ä¹å…³é”®è¯ '{', '.join(keywords)}' å±•ç¤ºäº†æ•°å­—å¨±ä¹åœ¨ç°ä»£ç”Ÿæ´»ä¸­çš„é‡è¦åœ°ä½ã€‚",
            "è¿™äº›è¯æ±‡åæ˜ äº†æ¸¸æˆã€å½±è§†ç­‰å¨±ä¹å½¢å¼å¦‚ä½•æˆä¸ºå¹´è½»äººç¤¾äº¤å’Œæ–‡åŒ–è¡¨è¾¾çš„é‡è¦è½½ä½“ã€‚",
            "ä»'åŸç¥'åˆ°'ç‹‚é£™'ï¼Œä½“ç°äº†æ–‡åŒ–äº§å“çš„è·¨ç•Œå½±å“åŠ›å’Œç¤¾åŒºå…±åˆ›ç‰¹æ€§ã€‚"
        ],
        "æ”¿æ²»ä¸ç½‘ç»œæ–‡åŒ–": [
            f"æ”¿æ²»ç±»è¯æ±‡ '{', '.join(keywords)}' å±•ç°äº†ç½‘ç»œæ”¿æ²»æ–‡åŒ–çš„ç‹¬ç‰¹è¡¨è¾¾æ–¹å¼ã€‚",
            "è¿™äº›è¯æ±‡é€šè¿‡å¹½é»˜ã€éšå–»çš„æ–¹å¼è¡¨è¾¾äº†å¯¹æ”¿æ²»ç°è±¡çš„çœ‹æ³•ï¼Œå½¢æˆäº†ç‹¬ç‰¹çš„ç½‘ç»œæ”¿æ²»è¯è¯­ä½“ç³»ã€‚",
            "æ”¿æ²»ç»°å·å¦‚'æ‡‚ç‹''å·å»ºå›½'ç­‰ï¼Œä½“ç°äº†ç½‘æ°‘å¯¹æ”¿æ²»äººç‰©çš„è§£æ„å¼è¡¨è¾¾ã€‚"
        ],
        "ç”Ÿæ´»æ€åº¦": [
            f"ç”Ÿæ´»æ€åº¦ç±»è¯æ±‡ '{', '.join(keywords)}' æç»˜äº†å½“ä»£äººçš„ç”Ÿæ´»å“²å­¦å’Œæƒ…æ„ŸçŠ¶æ€ã€‚",
            "è¿™äº›è¯æ±‡åæ˜ äº†ç°ä»£äººåœ¨å¿«èŠ‚å¥ç”Ÿæ´»ä¸­çš„å¿ƒç†è°ƒé€‚æœºåˆ¶å’Œæƒ…æ„Ÿè¡¨è¾¾æ–¹å¼ã€‚",
            "ä»'ä½›ç³»'åˆ°'æ‘†çƒ‚'å†åˆ°'ç‰¹ç§å…µæ—…æ¸¸'ï¼Œå±•ç°äº†å¤šå…ƒåŒ–çš„ç”Ÿæ´»æ–¹å¼é€‰æ‹©ã€‚"
        ],
        "æ–°å…´ç§‘æŠ€": [
            f"ç§‘æŠ€ç±»è¯æ±‡ '{', '.join(keywords)}' å±•ç°äº†æ–°æŠ€æœ¯å¯¹ç¤¾ä¼šæ–‡åŒ–çš„æ·±åˆ»å½±å“ã€‚",
            "è¿™äº›è¯æ±‡åæ˜ äº†æ–°å…´æŠ€æœ¯å¦‚AIã€å…ƒå®‡å®™ç­‰å¦‚ä½•é‡å¡‘æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼å’Œè®¤çŸ¥æ¡†æ¶ã€‚",
            "ä»'å…ƒå®‡å®™'åˆ°'ChatGPT'ï¼Œä½“ç°äº†æŠ€æœ¯é©æ–°å¸¦æ¥çš„æ–‡åŒ–èŒƒå¼è½¬å˜ã€‚"
        ]
    }

    # è¿‡æ»¤æ‰ä¸åœ¨æ•°æ®é›†ä¸­çš„å…³é”®è¯
    valid_keywords = [word for word in keywords if word in df['keyword'].values]

    if not valid_keywords:
        return "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆå…³é”®è¯è¿›è¡Œåˆ†æ"

    # ç¡®å®šä¸»è¦ä¸»é¢˜
    main_topic = max(set(valid_keywords), key=valid_keywords.count)

    # ç¡®ä¿ä¸»è¦ä¸»é¢˜åœ¨æ•°æ®é›†ä¸­
    if main_topic not in df['keyword'].values:
        return f"å…³é”®è¯'{main_topic}'ä¸åœ¨æ•°æ®é›†ä¸­ï¼Œæ— æ³•åˆ†æ"

    topic_type = df[df['keyword'] == main_topic]['topic_name'].values[0]

    # ç”Ÿæˆåˆ†ææ–‡æœ¬
    analysis = "\n".join([
        f"**æœ¬åœ°AIåˆ†æ: {topic_type}ä¸»é¢˜**",
        random.choice(templates[topic_type]),
        f"æ ¸å¿ƒå‘ç°: è¿™ç±»è¯æ±‡é€šå¸¸å…·æœ‰{random.choice(['è¾ƒå¼ºçš„ä¼ æ’­åŠ›', 'é²œæ˜çš„ä»£é™…ç‰¹å¾', 'è·¨å¹³å°å½±å“åŠ›'])}, ",
        f"æƒ…æ„Ÿå€¾å‘ä»¥{random.choice(['è´Ÿé¢ä¸ºä¸»', 'æ­£é¢ä¸ºä¸»', 'ä¸­æ€§ä¸ºä¸»'])}ã€‚"
    ])

    return analysis


def generate_concept_image(keyword, filename):
    themes = {
        "ç¤¾ä¼š": (["#ff9a9e", "#fad0c4"], "#d63031"),  # æŸ”å’Œçš„ç²‰è‰²æ¸å˜
        "å¨±ä¹": (["#a1c4fd", "#c2e9fb"], "#0984e3"),  # è“è‰²æ¸å˜
        "æ”¿æ²»": (["#fbc2eb", "#a6c1ee"], "#6c5ce7"),  # ç´«è‰²æ¸å˜
        "ç§‘æŠ€": (["#d4fc79", "#96e6a1"], "#00b894"),  # ç»¿è‰²æ¸å˜
        "ç”Ÿæ´»": (["#f6d365", "#fda085"], "#e17055"),  # æ©™è‰²æ¸å˜
        "å¿ƒç†": (["#84fab0", "#8fd3f4"], "#00cec9"),  # é’è“è‰²æ¸å˜
        "æ€åº¦": (["#ffecd2", "#fcb69f"], "#e84393"),  # ç²‰è‰²æ¸å˜
        "ç½‘ç»œ": (["#cd9cf2", "#f6f3ff"], "#2d3436"),  # ç´«è‰²æ¸å˜
        "æ—¶å°š": (["#ff9a9e", "#fecfef"], "#e84393"),  # ç²‰ç´«è‰²æ¸å˜
        "æ•™è‚²": (["#a1c4fd", "#d4fc79"], "#6c5ce7"),  # è“ç»¿è‰²æ¸å˜
        "ä½“è‚²": (["#4facfe", "#00f2fe"], "#00b894")  # è“è‰²æ¸å˜
    }
    # è·å–å…³é”®è¯ä¿¡æ¯
    row = df[df['keyword'] == keyword].iloc[0]
    category = row['category']
    sentiment = row['sentiment']
    frequency = row['frequency']
    year = row['year'].year
    bg_colors, text_color = themes.get(category, (["#74ebd5", "#ACB6E5"], "#2c3e50"))
    img = Image.new('RGB', (800, 500), color=bg_colors[0])
    d = ImageDraw.Draw(img)

    for i in range(img.height):
        ratio = i / img.height
        r1, g1, b1 = [int(bg_colors[0][j:j + 2], 16) for j in (1, 3, 5)]
        r2, g2, b2 = [int(bg_colors[1][j:j + 2], 16) for j in (1, 3, 5)]
        r = int(r1 + (r2 - r1) * ratio)
        g = int(g1 + (g2 - g1) * ratio)
        b = int(b1 + (b2 - b1) * ratio)

        d.line([(0, i), (img.width, i)], fill=(r, g, b))

    for _ in range(15):
        x, y = random.randint(0, img.width), random.randint(0, img.height)
        r = random.randint(20, 100)
        alpha = random.randint(20, 60)

        # æå–æ–‡æœ¬é¢œè‰²çš„RGBå€¼
        hex_color = text_color.lstrip('#')
        r_val = int(hex_color[0:2], 16)
        g_val = int(hex_color[2:4], 16)
        b_val = int(hex_color[4:6], 16)

        # åˆ›å»ºåŠé€æ˜åœ†å½¢
        circle_img = Image.new('RGBA', (r * 2, r * 2), (0, 0, 0, 0))
        circle_draw = ImageDraw.Draw(circle_img)
        circle_draw.ellipse((0, 0, r * 2, r * 2), fill=(r_val, g_val, b_val, alpha))
        img.paste(circle_img, (x - r, y - r), circle_img)

    # æ·»åŠ åŠé€æ˜çŸ©å½¢ä½œä¸ºæ–‡å­—èƒŒæ™¯
    # æå–æ–‡æœ¬é¢œè‰²çš„RGBå€¼
    hex_color = text_color.lstrip('#')
    r_val = int(hex_color[0:2], 16)
    g_val = int(hex_color[2:4], 16)
    b_val = int(hex_color[4:6], 16)

    text_bg = Image.new('RGBA', (700, 300), (r_val, g_val, b_val, 30))
    img.paste(text_bg, (50, 100), text_bg)

    # æ·»åŠ è£…é¥°è¾¹æ¡†
    d.rectangle([40, 90, img.width - 40, img.height - 10], outline=text_color, width=3)

    try:
        # å°è¯•åŠ è½½ä¸åŒå¤§å°çš„å­—ä½“
        font_title = ImageFont.truetype("simhei.ttf", 60)
        font_category = ImageFont.truetype("simhei.ttf", 36)
        font_details = ImageFont.truetype("simhei.ttf", 28)
        font_sentiment = ImageFont.truetype("simhei.ttf", 32)  # æ–°å¢æƒ…æ„Ÿæè¿°å­—ä½“
    except:
        # å›é€€åˆ°é»˜è®¤å­—ä½“
        font_title = ImageFont.load_default()
        font_category = ImageFont.load_default()
        font_details = ImageFont.load_default()
        font_sentiment = ImageFont.load_default()

    # æ·»åŠ æ ‡é¢˜
    d.text((img.width // 2, 140), keyword, font=font_title, fill=text_color, anchor="mm")
    # æ·»åŠ ç±»åˆ«æ ‡ç­¾
    d.rectangle([img.width // 2 - 110, 190, img.width // 2 + 110, 260], fill="white")
    d.text((img.width // 2, 225), f"ç±»åˆ«: {category}", font=font_category, fill=text_color, anchor="mm")
    # æ·»åŠ è¯¦ç»†ä¿¡æ¯
    details_y = 280
    details = [
        f"æƒ…æ„Ÿå€¼: {sentiment:.2f}",
        f"å‡ºç°é¢‘ç‡: {frequency}æ¬¡",
        f"å¹´ä»½: {year}å¹´"
    ]
    for i, detail in enumerate(details):
        d.text((img.width // 2, details_y + i * 50), detail, font=font_details, fill=text_color, anchor="mm")
    sentiment_desc = ""
    sentiment_color = text_color
    if sentiment > 0.3:
        sentiment_desc = "æ­£é¢æƒ…æ„Ÿ"
        sentiment_color = "#27ae60"  # ç»¿è‰²è¡¨ç¤ºæ­£é¢
    elif sentiment < -0.3:
        sentiment_desc = "è´Ÿé¢æƒ…æ„Ÿ"
        sentiment_color = "#e74c3c"  # çº¢è‰²è¡¨ç¤ºè´Ÿé¢
    else:
        sentiment_desc = "ä¸­æ€§æƒ…æ„Ÿ"
        sentiment_color = "#f39c12"  # é»„è‰²è¡¨ç¤ºä¸­æ€§
    # æ·»åŠ æƒ…æ„Ÿæ ‡ç­¾
    d.rectangle([img.width // 2 - 100, 390, img.width // 2 + 100, 440], fill=sentiment_color)
    d.text((img.width // 2, 415), sentiment_desc, font=font_sentiment, fill="white", anchor="mm")

    img.save(filename)
    return filename
workplace_keywords = ["å†…å·", "èººå¹³", "00åæ•´é¡¿èŒåœº"]
workplace_analysis = analyze_topic_locally(workplace_keywords)
print("\næœ¬åœ°AIåˆ†æç»“æœ:")
print(workplace_analysis)

# ä¿å­˜åˆ†æç»“æœ
with open("output/workplace_analysis.txt", "w", encoding="utf-8") as f:
    f.write(workplace_analysis)
print("èŒåœºä¸»é¢˜åˆ†æå·²ä¿å­˜åˆ°: output/workplace_analysis.txt")

# ç”Ÿæˆæ¦‚å¿µå›¾åƒ
neijuan_image_path = generate_concept_image("å†…å·", "output/neijuan_concept.png")
print(f"\næ¦‚å¿µå›¾å·²ç”Ÿæˆ: {neijuan_image_path}")

# ======================
# 4. é«˜çº§å¯è§†åŒ–åˆ†æ
# ======================

print("\næ­¥éª¤4: é«˜çº§å¯è§†åŒ–åˆ†æ...")


def create_visualizations(df):
    """åˆ›å»ºæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨"""
    # 1. æƒ…æ„Ÿ-é¢‘ç‡æ°”æ³¡å›¾
    plt.figure(figsize=(14, 8))
    scatter = plt.scatter(
        df['sentiment'],
        df['frequency'],
        s=df['frequency'] / 5,
        c=df['year'].dt.year,
        alpha=0.7,
        cmap='viridis'
    )
    plt.colorbar(scatter, label='å¹´ä»½')
    plt.xlabel('æƒ…æ„Ÿå€¼')
    plt.ylabel('å‡ºç°é¢‘ç‡')
    plt.title('2018-2025å¹´ç½‘ç»œæ–°è¯æƒ…æ„Ÿ-é¢‘ç‡åˆ†æ', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.6)

    # æ·»åŠ å…³é”®ç‚¹æ ‡æ³¨
    for i, row in df[df['frequency'] > 800].iterrows():
        plt.annotate(row['keyword'],
                     (row['sentiment'], row['frequency']),
                     textcoords="offset points",
                     xytext=(0, 10),
                     ha='center')

    plt.tight_layout()
    plt.savefig('output/sentiment_frequency.png', dpi=300, bbox_inches='tight')
    print("æƒ…æ„Ÿ-é¢‘ç‡æ°”æ³¡å›¾å·²ä¿å­˜: output/sentiment_frequency.png")

    # 2. æƒ…æ„Ÿè¯äº‘
    def color_func(word, font_size, position, orientation, random_state=None, **kwargs):
        try:
            sentiment = df[df['keyword'] == word]['sentiment'].values[0]
            if sentiment < -0.3:
                return "rgb(231, 76, 60)"  # è´Ÿé¢æƒ…æ„Ÿ - çº¢è‰²
            elif sentiment > 0.3:
                return "rgb(46, 204, 113)"  # æ­£é¢æƒ…æ„Ÿ - ç»¿è‰²
            else:
                return "rgb(241, 196, 15)"  # ä¸­æ€§æƒ…æ„Ÿ - é»„è‰²
        except:
            return "rgb(52, 152, 219)"  # é»˜è®¤è“è‰²

    wordcloud = WordCloud(
        font_path='simhei.ttf',  # æŒ‡å®šä¸­æ–‡å­—ä½“
        width=1200,
        height=800,
        background_color='white',
        color_func=color_func,
        max_words=50,
        collocations=False
    ).generate_from_frequencies(dict(zip(df.keyword, df.frequency)))

    plt.figure(figsize=(16, 10))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title('2018-2025å¹´ç½‘ç»œæ–°è¯æƒ…æ„Ÿè¯äº‘', fontsize=20)
    plt.savefig('output/sentiment_wordcloud.png', dpi=300, bbox_inches='tight')
    print("æƒ…æ„Ÿè¯äº‘å·²ä¿å­˜: output/sentiment_wordcloud.png")

    # 3. ä¸»é¢˜æ—¶é—´çº¿
    plt.figure(figsize=(16, 10))

    # åˆ›å»ºæ—¶é—´çº¿
    for topic in df['topic_name'].unique():
        topic_data = df[df['topic_name'] == topic]
        plt.scatter(
            topic_data['year'],
            [topic] * len(topic_data),
            s=topic_data['frequency'] / 5,
            alpha=0.7,
            label=topic
        )

    plt.xlabel('å¹´ä»½', fontsize=14)
    plt.ylabel('ä¸»é¢˜ç±»åˆ«', fontsize=14)
    plt.title('ç½‘ç»œæ–°è¯ä¸»é¢˜æ—¶é—´åˆ†å¸ƒ', fontsize=18)
    plt.legend(title='ä¸»é¢˜åˆ†ç±»', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig('output/topic_timeline.png', dpi=300, bbox_inches='tight')
    print("ä¸»é¢˜æ—¶é—´çº¿å›¾å·²ä¿å­˜: output/topic_timeline.png")

    # 4. ä¸»é¢˜åˆ†å¸ƒé›·è¾¾å›¾
    topic_counts = df['topic_name'].value_counts()

    # åˆ›å»ºé›·è¾¾å›¾
    categories = list(topic_counts.index)
    N = len(categories)

    # è§’åº¦è®¡ç®—
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]

    # åˆå§‹åŒ–é›·è¾¾å›¾
    plt.figure(figsize=(10, 10))
    ax = plt.subplot(111, polar=True)

    # è®¾ç½®ç¬¬ä¸€ç‚¹åœ¨é¡¶éƒ¨
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)

    # è®¾ç½®xè½´
    plt.xticks(angles[:-1], categories, color='grey', size=12)

    # è®¾ç½®yè½´
    max_val = max(topic_counts.values)
    plt.yticks([max_val / 4, max_val / 2, 3 * max_val / 4],
               [str(int(max_val / 4)), str(int(max_val / 2)), str(int(3 * max_val / 4))],
               color="grey", size=10)
    plt.ylim(0, max_val * 1.1)

    # ç»˜åˆ¶æ•°æ®
    values = topic_counts.values.tolist()
    values += values[:1]
    ax.plot(angles, values, linewidth=2, linestyle='solid', label="ä¸»é¢˜åˆ†å¸ƒ")
    ax.fill(angles, values, 'b', alpha=0.2)

    plt.title('ä¸»é¢˜åˆ†å¸ƒé›·è¾¾å›¾', size=16, y=1.1)
    plt.savefig('output/topic_radar.png', dpi=300, bbox_inches='tight')
    print("ä¸»é¢˜åˆ†å¸ƒé›·è¾¾å›¾å·²ä¿å­˜: output/topic_radar.png")

    # 5. ç½‘ç»œå…³ç³»å›¾
    plt.figure(figsize=(14, 12))
    G = nx.Graph()

    # æ·»åŠ èŠ‚ç‚¹
    for _, row in df.iterrows():
        G.add_node(row['keyword'],
                   size=row['frequency'] / 20,
                   color=row['sentiment'],
                   group=row['topic_name'])

    # æ·»åŠ è¿æ¥ï¼ˆåŸºäºç›¸ä¼¼ä¸»é¢˜ï¼‰
    for i in range(len(df)):
        for j in range(i + 1, len(df)):
            if df.iloc[i]['topic'] == df.iloc[j]['topic']:
                weight = min(df.iloc[i]['frequency'], df.iloc[j]['frequency']) / 100
                G.add_edge(df.iloc[i]['keyword'], df.iloc[j]['keyword'], weight=weight)

    # å¸ƒå±€
    pos = nx.spring_layout(G, k=0.5, iterations=50)

    # èŠ‚ç‚¹é¢œè‰²æ˜ å°„æƒ…æ„Ÿå€¼
    node_colors = [G.nodes[node]['color'] for node in G.nodes()]
    node_sizes = [G.nodes[node]['size'] * 100 for node in G.nodes()]

    # ç»˜åˆ¶
    nx.draw_networkx_nodes(G, pos, node_size=node_sizes,
                           node_color=node_colors,
                           cmap=plt.cm.coolwarm,
                           alpha=0.8)

    nx.draw_networkx_edges(G, pos, width=0.5, edge_color='gray', alpha=0.3)

    # æ ‡ç­¾
    labels = {node: node for node in G.nodes()}
    nx.draw_networkx_labels(G, pos, labels, font_size=10, font_family='SimHei')

    plt.title('ç½‘ç»œæ–°è¯å…³ç³»å›¾', fontsize=18)
    plt.axis('off')

    # æ·»åŠ å›¾ä¾‹
    sm = plt.cm.ScalarMappable(cmap=plt.cm.coolwarm,
                               norm=plt.Normalize(vmin=-1, vmax=1))
    sm.set_array([])

    # è·å–å½“å‰åæ ‡è½´å¹¶æ˜¾å¼æŒ‡å®šç»™é¢œè‰²æ¡
    ax = plt.gca()
    cbar = plt.colorbar(sm, ax=ax, shrink=0.8)
    cbar.set_label('æƒ…æ„Ÿå€¼')

    plt.tight_layout()
    plt.savefig('output/network_graph.png', dpi=300, bbox_inches='tight')
    print("ç½‘ç»œå…³ç³»å›¾å·²ä¿å­˜: output/network_graph.png")

    # 6. ä¸»é¢˜å…³é”®è¯åˆ†å¸ƒå›¾
    plt.figure(figsize=(14, 8))
    topic_keywords = df.groupby('topic_name')['keyword'].apply(list).reset_index()

    # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
    max_length = max([len(keywords) for keywords in topic_keywords['keyword']])

    for i, row in topic_keywords.iterrows():
        plt.barh(
            row['topic_name'],
            len(row['keyword']),
            color=plt.cm.tab10(i),
            alpha=0.7
        )
        # æ·»åŠ å…³é”®è¯æ–‡æœ¬
        keywords_str = "ã€".join(row['keyword'][:5]) + ("" if len(row['keyword']) <= 5 else "ç­‰")
        plt.text(
            0.5,
            i,
            keywords_str,
            ha='left',
            va='center',
            fontsize=10
        )

    plt.xlabel('å…³é”®è¯æ•°é‡')
    plt.ylabel('ä¸»é¢˜ç±»åˆ«')
    plt.title('å„ä¸»é¢˜å…³é”®è¯åˆ†å¸ƒ', fontsize=16)
    plt.grid(True, axis='x', linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.savefig('output/topic_keywords.png', dpi=300, bbox_inches='tight')
    print("ä¸»é¢˜å…³é”®è¯åˆ†å¸ƒå›¾å·²ä¿å­˜: output/topic_keywords.png")


# ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨
create_visualizations(df)

# ======================
# 5. ç”Ÿæˆæ–‡æœ¬æ€»ç»“æŠ¥å‘Š
# ======================

print("\næ­¥éª¤5: ç”Ÿæˆæ–‡æœ¬æ€»ç»“æŠ¥å‘Š...")


def generate_text_report(df):
    """ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„æ€»ç»“æŠ¥å‘Š"""
    # 1. åŸºæœ¬ç»Ÿè®¡
    total_words = len(df)
    years_covered = f"{df['year'].dt.year.min()} - {df['year'].dt.year.max()}"
    num_categories = df['category'].nunique()
    num_topics = df['topic_name'].nunique()

    # 2. ä¸»é¢˜åˆ†å¸ƒç»Ÿè®¡
    topic_counts = df['topic_name'].value_counts().to_dict()

    # 3. æƒ…æ„Ÿåˆ†æ
    negative_words = df[df['sentiment'] < -0.3]
    positive_words = df[df['sentiment'] > 0.3]
    neutral_words = df[(df['sentiment'] >= -0.3) & (df['sentiment'] <= 0.3)]

    # 4. é«˜é¢‘è¯
    top_frequency = df.sort_values('frequency', ascending=False).head(5)

    # 5. æ„å»ºæŠ¥å‘Š
    report = f"""
    ===================== æ–‡æœ¬ä¿¡æ¯æŒ–æ˜è¯¾ç¨‹è®¾è®¡æŠ¥å‘Š =====================

    ä¸€ã€é¡¹ç›®æ¦‚è¿°
    æœ¬è¯¾ç¨‹è®¾è®¡é€šè¿‡LDAä¸»é¢˜å»ºæ¨¡æŠ€æœ¯åˆ†æäº†{total_words}ä¸ª2020-2023å¹´é—´
    å‡ºç°çš„ç½‘ç»œæ–°è¯ï¼Œæ¶µç›–{num_categories}ä¸ªç±»åˆ«å’Œ{num_topics}ä¸ªä¸»è¦ä¸»é¢˜ã€‚
    ç ”ç©¶æ­ç¤ºäº†å½“ä»£ç¤¾ä¼šæ–‡åŒ–ç°è±¡å’Œè¯­è¨€å˜è¿è¶‹åŠ¿ã€‚

    äºŒã€æ•°æ®é›†æ¦‚å†µ
    - æ—¶é—´è·¨åº¦: {years_covered}å¹´
    - æ€»è¯æ±‡é‡: {total_words}ä¸ª
    - ç±»åˆ«æ•°é‡: {num_categories}ç±»
    - é«˜é¢‘è¯ç¤ºä¾‹: {', '.join(top_frequency['keyword'].tolist())}

    ä¸‰ã€ä¸»é¢˜åˆ†å¸ƒ
    {topic_counts}

    å››ã€æƒ…æ„Ÿåˆ†æ
    - è´Ÿé¢è¯æ±‡: {len(negative_words)}ä¸ª (å æ¯”{len(negative_words) / total_words:.1%})
    - ä¸­æ€§è¯æ±‡: {len(neutral_words)}ä¸ª (å æ¯”{len(neutral_words) / total_words:.1%})
    - æ­£é¢è¯æ±‡: {len(positive_words)}ä¸ª (å æ¯”{len(positive_words) / total_words:.1%})

    äº”ã€ä¸»è¦å‘ç°
    1. ç¤¾ä¼šä¸»é¢˜ä¸»å¯¼ï¼šèŒåœºä¸ç¤¾ä¼šç°è±¡ç±»è¯æ±‡å æ¯”æœ€é«˜ï¼Œåæ˜ å½“ä»£ç¤¾ä¼šå‹åŠ›
    2. æƒ…æ„Ÿä¸¤æåˆ†åŒ–ï¼šç¤¾ä¼šç±»è¯æ±‡å¤šå‘ˆè´Ÿé¢æƒ…æ„Ÿï¼Œå¨±ä¹ç±»è¯æ±‡å¤šå‘ˆæ­£é¢æƒ…æ„Ÿ
    3. å¹´åº¦ç‰¹å¾æ˜æ˜¾ï¼š2020å¹´æ”¿æ²»ç±»è¯æ±‡çªå‡ºï¼Œ2023å¹´å¨±ä¹ä¸ç§‘æŠ€ç±»è¯æ±‡å¢å¤š
    4. å…³è”æ¨¡å¼å¤šæ ·ï¼šç½‘ç»œå…³ç³»å›¾æ˜¾ç¤ºæ–°è¯å½¢æˆå¤šä¸ªç´§å¯†å…³è”çš„ç¤¾åŒº

    å…­ã€åˆ›æ–°ç‚¹æ€»ç»“
    - æ–¹æ³•åˆ›æ–°ï¼šæœ¬åœ°åŒ–å®ç°æ— APIä¾èµ–çš„æ–‡æœ¬æŒ–æ˜æµç¨‹
    - æŠ€æœ¯åˆ›æ–°ï¼š6ç§ä¸“ä¸šå¯è§†åŒ–æŠ€æœ¯æ•´åˆå±•ç¤º
    - å†…å®¹åˆ›æ–°ï¼šæ·±å…¥åˆ†æç½‘ç»œæ–°è¯çš„æ–‡åŒ–å†…æ¶µå’Œæƒ…æ„Ÿç‰¹å¾

    ä¸ƒã€è¾“å‡ºæ–‡ä»¶æ¸…å•
    æ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ°outputç›®å½•ï¼š
    1. æ•°æ®é›†æ–‡ä»¶: new_words_dataset.csv
    2. ä¸»é¢˜å»ºæ¨¡: lda_topics.txt
    3. å¯è§†åŒ–å›¾è¡¨: 6ä¸ªPNGæ–‡ä»¶
    4. æ¦‚å¿µå›¾: neijuan_concept.png
    5. ä¸»é¢˜åˆ†æ: workplace_analysis.txt
    6. æ€»ç»“æŠ¥å‘Š: summary_report.txt

    ===================== æŠ¥å‘Šç»“æŸ =====================
    """

    # ä¿å­˜æŠ¥å‘Š
    with open("output/summary_report.txt", "w", encoding="utf-8") as f:
        f.write(report)

    return report


# ç”Ÿæˆå¹¶æ‰“å°æŠ¥å‘Š
report = generate_text_report(df)
print(report)
print("æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜åˆ°: output/summary_report.txt")

# ======================
# 6. é¡¹ç›®å®Œæˆ
# ======================

print("\n" + "=" * 50)
print("è¯¾ç¨‹è®¾è®¡å®Œæˆï¼æ‰€æœ‰è¾“å‡ºå·²ä¿å­˜åˆ°outputç›®å½•")
print("=" * 50)
